{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16a72c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Learning Round 1\n",
      "Federated Learning Round 2\n",
      "Federated Learning Round 3\n",
      "Federated Learning Round 4\n",
      "Federated Learning Round 5\n",
      "Accuracy of the global model on the test data: 92.51%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Define the model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # First fully connected layer: input size is 28*28 (MNIST image size), output size is 128\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "        self.fc2 = nn.Linear(128, 10)  # Second fully connected layer: input size is 128, output size is 10 (number of classes in MNIST)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax function applied along the dimension 1 (class scores)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input tensor to a 1D vector\n",
    "        x = self.fc1(x)  # Apply the first fully connected layer\n",
    "        x = self.relu(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)  # Apply the second fully connected layer\n",
    "        return self.softmax(x)  # Apply the Softmax function to get class probabilities\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# Transform: convert images to tensors and normalize them with mean=0.5 and std=0.5\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Load the training dataset\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Load the test dataset\n",
    "\n",
    "# Simulate federated learning clients\n",
    "num_clients = 5  # Number of clients\n",
    "client_data_size = len(train_dataset) // num_clients  # Size of data each client will have\n",
    "clients = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    client_indices = list(range(i * client_data_size, (i + 1) * client_data_size))\n",
    "    # Select data indices for each client\n",
    "    clients.append(Subset(train_dataset, client_indices))\n",
    "    # Create a subset of the training data for the client\n",
    "\n",
    "# Federated learning process\n",
    "global_model = SimpleMLP()  # Initialize the global model\n",
    "\n",
    "def federated_avg(weights):\n",
    "    avg_weights = [torch.mean(torch.stack([client_weights[layer] for client_weights in weights]), dim=0)\n",
    "                   for layer in range(len(weights[0]))]\n",
    "    # Compute the average weights across all clients for each layer\n",
    "    return avg_weights\n",
    "\n",
    "num_rounds = 5  # Number of federated learning rounds\n",
    "for round_num in range(num_rounds):\n",
    "    print(f'Federated Learning Round {round_num + 1}')\n",
    "    client_weights = []  # List to store weights from all clients\n",
    "    \n",
    "    for client_data in clients:\n",
    "        model = SimpleMLP()  # Initialize a new model for each client\n",
    "        model.load_state_dict(global_model.state_dict())  # Load the current global model's weights\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "        criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification\n",
    "\n",
    "        dataloader = DataLoader(client_data, batch_size=64, shuffle=True)\n",
    "        # DataLoader for the client's data with batch size of 64\n",
    "\n",
    "        model.train()  # Set model to training mode\n",
    "        for epoch in range(1):  # Train for 1 epoch\n",
    "            for x_client, y_client in dataloader:\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "                output = model(x_client)  # Forward pass\n",
    "                loss = criterion(output, y_client)  # Compute loss\n",
    "                loss.backward()  # Backward pass (compute gradients)\n",
    "                optimizer.step()  # Update model weights\n",
    "        \n",
    "        client_weights.append([param.data.clone() for param in model.parameters()])\n",
    "        # Store the model's weights after training on client data\n",
    "    \n",
    "    # Aggregate weights\n",
    "    new_weights = federated_avg(client_weights)\n",
    "    with torch.no_grad():  # Update global model weights without tracking gradients\n",
    "        for param, new_weight in zip(global_model.parameters(), new_weights):\n",
    "            param.data = new_weight\n",
    "\n",
    "# Evaluate the global model on the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "# DataLoader for the test data with batch size of 1000\n",
    "\n",
    "global_model.eval()  # Set global model to evaluation mode\n",
    "correct = 0  # Counter for correct predictions\n",
    "total = 0  # Counter for total predictions\n",
    "with torch.no_grad():  # No gradient computation needed\n",
    "    for x_test, y_test in test_loader:\n",
    "        output = global_model(x_test)  # Forward pass\n",
    "        _, predicted = torch.max(output.data, 1)  # Get the class with the highest score\n",
    "        total += y_test.size(0)  # Increment total by the batch size\n",
    "        correct += (predicted == y_test).sum().item()  # Count the number of correct predictions\n",
    "print(f'Accuracy of the global model on the test data: {100 * correct / total:.2f}%')\n",
    "# Print the accuracy of the global model on the test data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
